{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### 10 Emoji Prediction\n",
    "\n",
    "The task of this project is to make a system that would automatically fill the text with the appropriate emoticons. This can be done in two steps. First, for each position within the text a prediction is made whether an emoticon should be placed there. Second, an appropriate emoticon is chosen from a list of available emoticons. Both these tasks can be set up as supervised classification problems.\n",
    "\n",
    "Competition website:\n",
    "https://competitions.codalab.org/competitions/17344\n",
    "\n",
    "Dataset:\n",
    "https://competitions.codalab.org/competitions/17344\n",
    "\n",
    "Entry point:\n",
    "https://arxiv.org/pdf/1702.07285.pdf (Barbieri, Francesco, Miguel Ballesteros, and Horacio Saggion. Are Emojis Predictable?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Define paths to folders containing data and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# NUMBER_OF_TWEETS = None\n",
    "NUMBER_OF_TWEETS = 100000\n",
    "\n",
    "MAX_WORDS_PER_TWEET = 30\n",
    "DATA_LOCATION = \"./train/data/\"\n",
    "RESULT_LOCATION = \"./result/\"\n",
    "TWEET_FILE_NAME = \"tweet_by_ID_28_4_2018__03_20_05\" + \"_\"\n",
    "\n",
    "if NUMBER_OF_TWEETS is not None:\n",
    "    TWEET_FILE_NAME += str(NUMBER_OF_TWEETS)\n",
    "else:\n",
    "    TWEET_FILE_NAME += \"ALL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Load the data\n",
    "Tweets are loaded in two ways: list of strings (for the TF-IDF vectorizer) and a list of list of words (for feature extraction). Labels are read as a numpy array of N * MAX_WORDS_PER_TWEET dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example of tweet texts:\n",
      "['lol west covina california', 'things got a little festive at the office christmas2016 redrock', 'step out and explore ellis island cafe', 'rupauls drag race bingo fun drag queens be sexy rupaulsdragrace user abwyman la', 'just light makeup blueeyes lupusgirl photography modelingagency modeling smiling']\n",
      "\n",
      "example of labels (emoji locations):\n",
      "[[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "base_file_name = DATA_LOCATION + TWEET_FILE_NAME\n",
    "\n",
    "text_lines = []\n",
    "text_lines_split = []\n",
    "\n",
    "with open(base_file_name + \".text\", 'r', encoding=\"utf-8\") as out_text:\n",
    "    for line in out_text:\n",
    "        text_lines.append(line[:-1])\n",
    "        text_lines_split.append(line[:-1].split())\n",
    "        \n",
    "loc_lines = []\n",
    "with open(base_file_name + \".loclabels\", 'r') as loc_labels:\n",
    "    for line in loc_labels:\n",
    "        loc_line = []\n",
    "        for c in line[:-1]:\n",
    "            loc_line.append(int(c))\n",
    "        loc_lines.append(loc_line)\n",
    "\n",
    "loc_lines = np.asarray(loc_lines)\n",
    "\n",
    "# full_text = open(base_file_name + \".full\", 'r')\n",
    "# emoji_labels = open(base_file_name + \".emolabels\", 'r')\n",
    "# emoji_ids = open(base_file_name + \".ids\", 'r')\n",
    "\n",
    "print(f\"example of tweet texts:\\n{text_lines[:5]}\\n\")\n",
    "print(f\"example of labels (emoji locations):\\n{loc_lines[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### TF-IDF Feature Extraction\n",
    "TF-IDF is computed on the collection of tweets. Then for every position between words a new example is generated: a 2 * k array containing the k left and k right tfidf values of words. Labels are taken as 1 or 0 wether an emoji was there in the original tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import random\n",
    "\n",
    "def tfidf_features(tweets, labels, k, func):\n",
    "    tfidf_model = TfidfVectorizer(input=\"content\", analyzer=\"word\", stop_words=\"english\")\n",
    "    X_tfidf = tfidf_model.fit_transform(text_lines)\n",
    "\n",
    "    word_to_tfidf_index_dict = {}\n",
    "    for i, word in enumerate(tfidf_model.get_feature_names()):\n",
    "        word_to_tfidf_index_dict[word] = i\n",
    "\n",
    "    print(f\"X shape {X_tfidf.shape}\")\n",
    "    print(f\"Y shape {loc_lines.shape}\")\n",
    "    print(f\"some tf-idf values\\n{X_tfidf[0]}\\n\")\n",
    "\n",
    "    N = len(tweets)\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for tweet_index, (tweet, label) in enumerate(zip(tweets, labels)):\n",
    "        for pos in range(len(tweet) + 1):\n",
    "                \n",
    "            x = []\n",
    "            for i in range(pos - k, pos + k):\n",
    "                if i < 0 or i >= len(tweet):\n",
    "                    x.append(0.0)\n",
    "                else:\n",
    "                    x.append(func(tweet_index, tweet[i]))\n",
    "            X.append(x)\n",
    "            y.append(label[pos])\n",
    "            \n",
    "    return np.asarray(X), np.asarray(y)\n",
    "\n",
    "def word_to_tfidf(tweet_index, word):\n",
    "    if word in word_to_tfidf_index_dict:\n",
    "        return X_tfidf[tweet_index, word_to_tfidf_index_dict[word]]\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def print_dataset(X, y):\n",
    "    emoji_num = np.count_nonzero(y)\n",
    "    class_freq_ratio = emoji_num / (X.shape[0] * X.shape[1])\n",
    "    \n",
    "    print(\"after feature extraction:\")\n",
    "    print(f\"X shape {X.shape}\")\n",
    "    print(f\"y shape {y.shape}\")\n",
    "    print(f\"some X values\\n{X[:5]}\")\n",
    "    print(f\"some y values\\n{y[:5]}\")\n",
    "    print(f\"non zero elements (1 in label) in y {emoji_num}\")\n",
    "    print(f\"class frequency ratio {class_freq_ratio}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vector Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "num_of_features = 6\n",
    "w2v_model_file_name = \"w2v_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(text_lines_split, min_count=1, size=num_of_features)\n",
    "w2v_model.save(w2v_model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,)\n",
      "[-1.1020933 -8.837424   1.1465905  5.0093884  0.1336154 -1.5663345]\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(w2v_model_file_name)\n",
    "\n",
    "example = w2v_model.wv[\"california\"]\n",
    "print(example.shape)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_features(tweets, labels, k):\n",
    "    N = len(tweets)\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for tweet_index, (tweet, label) in enumerate(zip(tweets, labels)):\n",
    "        for pos in range(len(tweet) + 1):\n",
    "            x = []\n",
    "            for i in range(pos - k, pos + k):\n",
    "                if i < 0 or i >= len(tweet):\n",
    "                    x.append(np.zeros(num_of_features))\n",
    "                else:\n",
    "                    x.append(w2v_model.wv[tweet[i]])\n",
    "            x = np.average(x, axis=0)\n",
    "            X.append(x)\n",
    "            y.append(label[pos])\n",
    "            \n",
    "    return np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after feature extraction:\n",
      "X shape (1187039, 12)\n",
      "y shape (1187039,)\n",
      "some X values\n",
      "[[ 0.          0.          0.          0.42782282  0.43669317  0.7050588\n",
      "  -0.20362148 -1.25866681  1.40187832  0.3100376  -0.58608462 -0.02237907]\n",
      " [ 0.          0.          0.42782282  0.43669317  0.7050588   0.3593867\n",
      "  -0.3873037  -2.73157086  1.59297673  1.14493567 -0.56381539 -0.28343482]\n",
      " [ 0.          0.42782282  0.43669317  0.7050588   0.3593867   0.\n",
      "  -0.3873037  -2.73157086  1.59297673  1.14493567 -0.56381539 -0.28343482]\n",
      " [ 0.42782282  0.43669317  0.7050588   0.3593867   0.          0.\n",
      "  -0.3873037  -2.73157086  1.59297673  1.14493567 -0.56381539 -0.28343482]\n",
      " [ 0.43669317  0.7050588   0.3593867   0.          0.          0.\n",
      "  -0.04176051 -2.21913538  0.87697594  0.99023006 -0.22808655 -0.87206299]]\n",
      "some y values\n",
      "[0 1 0 0 0]\n",
      "non zero elements (1 in label) in y 100779\n",
      "class frequency ratio 0.007074957099134907\n",
      "\n"
     ]
    }
   ],
   "source": [
    "should_use_tfidf = False\n",
    "should_use_hybrid = True\n",
    "k = 3\n",
    "\n",
    "# X_tfidf, y_tfidf = neighbor_features(text_lines_split, loc_lines, k, word_to_tfidf)\n",
    "# X_w2v, y_w2v = w2v_features(text_lines_split, loc_lines, k)\n",
    "X_hybrid = np.hstack((X_tfidf, X_w2v))\n",
    "\n",
    "if should_use_tfidf:\n",
    "    X, y = X_tfidf, y_tfidf\n",
    "else:\n",
    "    X, y = X_w2v, y_w2v\n",
    "    \n",
    "if should_use_hybrid:\n",
    "    X = X_hybrid\n",
    "\n",
    "print_dataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Build data sets\n",
    "Dataset is randomly split into train and test subsets. Ignored while using KFold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(949631, 6)\n",
      "(949631,)\n",
      "[[0.         0.21002416 0.46831285 0.         0.         0.35150307]\n",
      " [0.31119785 0.         0.32296707 0.23357677 0.27625077 0.3067234 ]\n",
      " [0.31842575 0.3052548  0.14581719 0.23862097 0.3573096  0.37329836]\n",
      " [0.34882237 0.27353302 0.4324465  0.29457364 0.26902211 0.        ]\n",
      " [0.         0.51469825 0.3309578  0.51469825 0.11501215 0.        ]\n",
      " [0.         0.         0.         0.21093655 0.         0.        ]\n",
      " [0.         0.46152608 0.31650171 0.58426773 0.33550733 0.        ]\n",
      " [0.16048372 0.2187436  0.47048244 0.47048244 0.47048244 0.40641934]\n",
      " [0.37097258 0.         0.54391678 0.40979628 0.         0.        ]\n",
      " [0.         0.         0.         0.48248596 0.         0.        ]\n",
      " [0.         0.25451798 0.23668705 0.         0.14669033 0.25408373]\n",
      " [0.5359691  0.         0.21991171 0.         0.22324085 0.        ]\n",
      " [0.32315036 0.         0.         0.24319833 0.         0.        ]\n",
      " [0.17264715 0.31196645 0.20075195 0.3607291  0.         0.        ]\n",
      " [0.3417698  0.         0.26336237 0.37049091 0.28560716 0.29992424]\n",
      " [0.         0.25859496 0.46411592 0.46411592 0.11242461 0.24412707]\n",
      " [0.34566482 0.         0.         0.48965049 0.41295971 0.        ]\n",
      " [0.35529269 0.30614581 0.58987255 0.55528069 0.         0.        ]\n",
      " [0.45963851 0.34180257 0.33192106 0.63867625 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.24277867 0.56937868]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if not should_use_kfold:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(X_train[30:50])\n",
    "    print(y_train[30:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Baselines\n",
    "\n",
    "Make sure to check should_train flags when training/testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "should_use_kfold = True\n",
    "\n",
    "should_train_global = False\n",
    "should_train_linear_svm = True\n",
    "should_train_bagging_svm = False\n",
    "should_train_random_forest = True\n",
    "should_train_adaboost = True\n",
    "\n",
    "linear_svm_model_file_name = \"linear_svm.pkl\"\n",
    "bagging_model_file_name = \"bagging_svm.pkl\"\n",
    "random_forest_file_name = \"random_forest.pkl\"\n",
    "adaboost_file_name = \"adaboost.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "def calc_scores(y_pred, y_test):\n",
    "    result = []\n",
    "    result.append(accuracy_score(y_pred, y_test))\n",
    "    result.append(precision_score(y_pred, y_test))\n",
    "    result.append(recall_score(y_pred, y_test))\n",
    "    result.append(f1_score(y_pred, y_test))\n",
    "    result.append(np.count_nonzero(y_pred) / y_pred.shape[0])\n",
    "    return result\n",
    "\n",
    "def print_scores(scores):\n",
    "    print(f\"Accuracy: {scores[0]}\")\n",
    "    print(f\"Precision: {scores[1]}\")\n",
    "    print(f\"Recall: {scores[2]}\")\n",
    "    print(f\"F1: {scores[3]}\")\n",
    "    print(f\"ratio of positive/negative predictions {scores[4]}\")\n",
    "\n",
    "def kfold_score(clf, X, y):\n",
    "    fold_scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train_kf, X_test_kf = X[train_index], X[test_index]\n",
    "        y_train_kf, y_test_kf = y[train_index], y[test_index]\n",
    "        clf.fit(X_train_kf, y_train_kf)\n",
    "        y_pred = clf.predict(X_test_kf)\n",
    "        fold_scores.append(calc_scores(y_pred, y_test_kf))\n",
    "    \n",
    "    fold_scores = np.average(np.asarray(fold_scores), axis=0)\n",
    "    print_scores(fold_scores)\n",
    "\n",
    "def train_and_save_model(model, X_train, y_train, model_file_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    joblib.dump(model, model_file_name)\n",
    "    \n",
    "def load_and_test_model(model_file_name, X_test, y_test):\n",
    "    clf_loaded = joblib.load(model_file_name)\n",
    "    y_pred = clf_loaded.predict(X_test)\n",
    "    print_scores(calc_scores(y_pred, y_test))\n",
    "    \n",
    "def do_test(clf, X, y, clf_file_name, flag):\n",
    "    if should_use_kfold:\n",
    "        kfold_score(clf, X, y)\n",
    "    else:\n",
    "        if should_train_global and flag:\n",
    "            train_and_save_model(clf, X_train, y_train, clf_file_name)\n",
    "        load_and_test_model(clf_file_name, X_test, y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Linear SVM Baseline\n",
    "\n",
    "Linear SVM is based on the liblinear library and is faster on large datasets. Not using dual optimization problem makes the train extremely fast. It is the fastest so both feature extraction methods are used for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF results:\n",
      "Accuracy: 0.6851072293143143\n",
      "Precision: 0.6788313465941525\n",
      "Recall: 0.16692653457176304\n",
      "F1: 0.2679578548316831\n",
      "ratio of positive/negative predictions 0.3452582428652209\n",
      "\n",
      "word2vec results:\n",
      "Accuracy: 0.6029279580526152\n",
      "Precision: 0.5861753932193298\n",
      "Recall: 0.12087777365935053\n",
      "F1: 0.20042450128911754\n",
      "ratio of positive/negative predictions 0.4117059328615028\n",
      "\n",
      "hybrid results:\n",
      "Accuracy: 0.6863826712665466\n",
      "Precision: 0.6786055863402377\n",
      "Recall: 0.16750723267072315\n",
      "F1: 0.26868920296183413\n",
      "ratio of positive/negative predictions 0.34394910411405605\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_clf = LinearSVC(class_weight=\"balanced\", dual=False)\n",
    "\n",
    "print(\"TF-IDF results:\")\n",
    "do_test(svm_clf, X_tfidf, y_tfidf, linear_svm_model_file_name, should_train_linear_svm)\n",
    "print(\"\\nword2vec results:\")\n",
    "do_test(svm_clf, X_w2v, y_w2v, linear_svm_model_file_name, should_train_linear_svm)\n",
    "print(\"\\nhybrid results:\")\n",
    "do_test(svm_clf, X_hybrid, y_w2v, linear_svm_model_file_name, should_train_linear_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Bagging SVM\n",
    "\n",
    "Warning! SVM is based on the libsvm library and it scales poorly with large datasets. That is why an ensemble (bagging) is used. Each classifier is trained on a portion of the data which greatly reduces training times and gives similar (if not better) results. Using 10 estimators it still takes a few hours to train. Results are just a bit better than a single linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6922597385092331\n",
      "Precision: 0.6711322169457635\n",
      "Recall: 0.16802032362621547\n",
      "F1: 0.268756505725038\n",
      "ratio of positive/negative predictions 0.336580907130341\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "n_estimators = 10\n",
    "bagging_svm_clf = BaggingClassifier(SVC(kernel='linear', class_weight='balanced'), max_samples=1.0 / n_estimators,\n",
    "                                        n_estimators=n_estimators, bootstrap=False)\n",
    "\n",
    "do_test(bagging_svm_clf, X, y, bagging_model_file_name, should_train_bagging_svm)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Random Forest Baseline\n",
    "\n",
    "Random forests are pretty fast, but the results are generally worse (F1). High accuracy and recall with low precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9147627712629735\n",
      "Precision: 0.12786803299175206\n",
      "Recall: 0.4783991023003553\n",
      "F1: 0.20179867466077625\n",
      "ratio of positive/negative predictions 0.022522408680415152\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest_clf = RandomForestClassifier(min_samples_leaf=2)\n",
    "\n",
    "do_test(random_forest_clf, X, y, random_forest_file_name, should_train_random_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### AdaBoost Baseline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7398234263377814\n",
      "Precision: 0.6800299925018746\n",
      "Recall: 0.19724231923562077\n",
      "F1: 0.30579032548102864\n",
      "ratio of positive/negative predictions 0.29051674754009976\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "adaboost_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, class_weight=\"balanced\"), n_estimators=100)\n",
    "\n",
    "do_test(adaboost_clf, X, y, adaboost_file_name, should_train_adaboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

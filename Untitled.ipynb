{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 Emoji Prediction\n",
    "\n",
    "The task of this project is to make a system that would automatically fill the text with the appropriate emoticons. This can be done in two steps. First, for each position within the text a prediction is made whether an emoticon should be placed there. Second, an appropriate emoticon is chosen from a list of available emoticons. Both these tasks can be set up as supervised classification problems.\n",
    "\n",
    "Competition website:\n",
    "https://competitions.codalab.org/competitions/17344\n",
    "\n",
    "Dataset:\n",
    "https://competitions.codalab.org/competitions/17344\n",
    "\n",
    "Entry point:\n",
    "https://arxiv.org/pdf/1702.07285.pdf (Barbieri, Francesco, Miguel Ballesteros, and Horacio Saggion. Are Emojis Predictable?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paths to folders containing data and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS_PER_TWEET = 30\n",
    "DATA_LOCATION = \"./train/crawler/data/\"\n",
    "RESULT_LOCATION = \"./result/\"\n",
    "TWEET_FILE_NAME = \"tweet_by_ID_28_4_2018__03_20_05.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "Tweets are loaded in two ways: list of strings (for the TF-IDF vectorizer) and a list of list of words (for feature extraction). Labels are read as a numpy array of N * MAX_WORDS_PER_TWEET dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lol west covina california', 'things got a little festive at the office christmas2016 redrock', 'step out and explore ellis island cafe', 'rupauls drag race bingo fun drag queens be sexy rupaulsdragrace user abwyman la', 'just light makeup blueeyes lupusgirl photography modelingagency modeling smiling']\n",
      "[[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "base_file_name = DATA_LOCATION + TWEET_FILE_NAME\n",
    "\n",
    "text_lines = []\n",
    "text_lines_split = []\n",
    "\n",
    "with open(base_file_name + \".text\", 'r', encoding=\"utf-8\") as out_text:\n",
    "    for line in out_text:\n",
    "        text_lines.append(line[:-1])\n",
    "        text_lines_split.append(line[:-1].split())\n",
    "        \n",
    "loc_lines = []\n",
    "with open(base_file_name + \".loclabels\", 'r') as loc_labels:\n",
    "    for line in loc_labels:\n",
    "        loc_line = []\n",
    "        for c in line[:-1]:\n",
    "            loc_line.append(int(c))\n",
    "        loc_lines.append(loc_line)\n",
    "\n",
    "loc_lines = np.asarray(loc_lines)\n",
    "\n",
    "# full_text = open(base_file_name + \".full\", 'r')\n",
    "# emoji_labels = open(base_file_name + \".emolabels\", 'r')\n",
    "# emoji_ids = open(base_file_name + \".ids\", 'r')\n",
    "\n",
    "print(text_lines[:5])\n",
    "print(loc_lines[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "TF-IDF is computed on the collection of tweets. Then for every position between words a new example is generated: a 2 * k array containing the k left and k right tfidf values of words. Labels are taken as 1 or 0 wether an emoji was there in the original tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 101169)\n",
      "  (0, 53222)\t0.4278228183080142\n",
      "  (0, 97113)\t0.43669316514848056\n",
      "  (0, 21796)\t0.7050588046593335\n",
      "  (0, 15621)\t0.35938669649827265\n",
      "[[0.         0.         0.         0.42782282 0.43669317 0.7050588 ]\n",
      " [0.         0.         0.42782282 0.43669317 0.7050588  0.3593867 ]\n",
      " [0.         0.42782282 0.43669317 0.7050588  0.3593867  0.        ]\n",
      " [0.42782282 0.43669317 0.7050588  0.3593867  0.         0.        ]\n",
      " [0.43669317 0.7050588  0.3593867  0.         0.         0.        ]]\n",
      "[0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "\n",
    "tfidf_model = TfidfVectorizer(input=\"content\", analyzer=\"word\", stop_words=\"english\")\n",
    "X_tfidf = tfidf_model.fit_transform(text_lines)\n",
    "\n",
    "word_to_tfidf_index_dict = {}\n",
    "for i, word in enumerate(tfidf_model.get_feature_names()):\n",
    "    word_to_tfidf_index_dict[word] = i\n",
    "\n",
    "print(X_tfidf.shape)\n",
    "print(X_tfidf[0])\n",
    "\n",
    "def neighbor_features(tweets, labels, k, func):\n",
    "    N = len(tweets)\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for tweet_index, (tweet, label) in enumerate(zip(tweets, labels)):\n",
    "        for pos in range(len(tweet) + 1):\n",
    "            x = []\n",
    "            for i in range(pos - k, pos + k):\n",
    "                if i < 0 or i >= len(tweet):\n",
    "                    x.append(0.0)\n",
    "                else:\n",
    "                    x.append(func(tweet_index, tweet[i]))\n",
    "            X.append(x)\n",
    "            y.append(label[pos])\n",
    "            \n",
    "    return np.asarray(X), np.asarray(y)\n",
    "\n",
    "def word_to_tfidf(tweet_index, word):\n",
    "    if word in word_to_tfidf_index_dict:\n",
    "        return X_tfidf[tweet_index, word_to_tfidf_index_dict[word]]\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "X, y = neighbor_features(text_lines_split, loc_lines, 3, word_to_tfidf)\n",
    "\n",
    "print(X[:5])\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build data sets\n",
    "Dataset is randomly split into train and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.21002416 0.46831285 0.         0.         0.35150307]\n",
      " [0.31119785 0.         0.32296707 0.23357677 0.27625077 0.3067234 ]\n",
      " [0.31842575 0.3052548  0.14581719 0.23862097 0.3573096  0.37329836]\n",
      " [0.34882237 0.27353302 0.4324465  0.29457364 0.26902211 0.        ]\n",
      " [0.         0.51469825 0.3309578  0.51469825 0.11501215 0.        ]\n",
      " [0.         0.         0.         0.21093655 0.         0.        ]\n",
      " [0.         0.46152608 0.31650171 0.58426773 0.33550733 0.        ]\n",
      " [0.16048372 0.2187436  0.47048244 0.47048244 0.47048244 0.40641934]\n",
      " [0.37097258 0.         0.54391678 0.40979628 0.         0.        ]\n",
      " [0.         0.         0.         0.48248596 0.         0.        ]\n",
      " [0.         0.25451798 0.23668705 0.         0.14669033 0.25408373]\n",
      " [0.5359691  0.         0.21991171 0.         0.22324085 0.        ]\n",
      " [0.32315036 0.         0.         0.24319833 0.         0.        ]\n",
      " [0.17264715 0.31196645 0.20075195 0.3607291  0.         0.        ]\n",
      " [0.3417698  0.         0.26336237 0.37049091 0.28560716 0.29992424]\n",
      " [0.         0.25859496 0.46411592 0.46411592 0.11242461 0.24412707]\n",
      " [0.34566482 0.         0.         0.48965049 0.41295971 0.        ]\n",
      " [0.35529269 0.30614581 0.58987255 0.55528069 0.         0.        ]\n",
      " [0.45963851 0.34180257 0.33192106 0.63867625 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.24277867 0.56937868]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train[30:50])\n",
    "print(y_train[30:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from numpy.linalg import norm\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "model_file_name = \"svm.pkl\"\n",
    "\n",
    "svm_clf = SVC(kernel=\"rbf\")\n",
    "svm_clf.fit(X_train, y_train)\n",
    "joblib.dump(svm_clf, model_file_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "svm_clf = joblib.load(model_file_name) \n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "accuracy_result = accuracy_score(y_pred, y_test)\n",
    "precision_result = precision_score(y_pred, y_test)\n",
    "recall_result = recall_score(y_pred, y_test)\n",
    "f1_result = f1_score(y_pred, y_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_result}\")\n",
    "print(f\"Precision: {precision_result}\")\n",
    "print(f\"Recall: {recall_result}\")\n",
    "print(f\"F1: {f1_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.count_nonzero(y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
